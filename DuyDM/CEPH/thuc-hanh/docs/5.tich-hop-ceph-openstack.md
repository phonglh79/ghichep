# Ghi chép lại các bước tích hợp CEPH - Openstack

### Mục lục

**[1. Mô hình triển khai](#mohinh)<br>**
**[2. IP Planning](#planning)<br>**
**[3. Các bước thực hiện tích hợp](#step)<br>**
   [3.1. Chuẩn bị tích hợp](#chuanbi)<br>
   [3.2. Tích hợp CEPH làm backend cho glance - images](#glance)<br>
   [3.3. Tích hợp CEPH làm backend cho cinder-volume và cinder-backup](#cinder)<br>
   [3.3. Tích hợp CEPH làm backend cho nova-compute](#nova)<br>

<a name="mohinh"></a>
## 1. Mô hình triển khai

![](../images/tich-hop-ceph-op/topocephop.png)

- Triển khai trên môi trường ảo hóa VMware ESXi.

- CEPH gồm 3 node CEPH mỗi node có 3 OSDs. Tham khảo cài đặt <a href="https://github.com/domanhduy/ghichep/blob/master/DuyDM/CEPH/thuc-hanh/docs/2.huong-dan-cai-dat-ceph-luminous.md" target="_blank">tại đây</a>!

- Openstack gồm 1 node Controller, 2 node Compute. Tham khảo cài đặt <a href="https://github.com/domanhduy/ghichep/blob/master/DuyDM/Openstack/install-openstack/docs/manual.md" target="_blank">tại đây</a>!

<a name="planning"></a>
## 2. IP Planning

![](../images/tich-hop-ceph-op/Screenshot_1701.png)


<a name="step"></a>
## 3. Các bước thực hiện tích hợp

Về cơ bản Openstack có 3 service có thể tích hợp để lưu trữ xuống dưới CEPH.

- Glance (images): Lưu trữ các images xuống CEPH.

- Cinder (volume): Lưu trữ các ổ đĩa của máy ảo được tạo ra trên Openstack xuống dưới CEPH.

- Nova (conmpute): Mặc định khi VM được tạo sẽ sinh ra một file lưu thông tin file disk của VM đó trên chính node compute đó, có thể tích hợp xuống CEPH thông qua một `symlink`.

<a name="chuanbi"></a>
### 3.1. Chuẩn bị

Thực hiện chung cho việc tích hợp các thành phần Openstack vào CEPH.

**Cài đặt lib ceph python cho các node CTL, COM**

python-rbd: Thư viện hỗ trợ ceph.

ceph-common: Kết nối client vào ceph cluster.

```
yum install -y python-rbd ceph-common
```

**Tạo pool trên CEPH (Thực hiện trên node CEPH)**

Tùy thuộc và nhu cầu tích hợp để tạo những pool khác nhau với mục đích quản lý khác nhau. Sử dụng công cụ tính toán pool do CEPH hỗ trợ.

https://ceph.com/pgcalc/


![](../images/tich-hop-ceph-op/Screenshot_1702.png)

![](../images/tich-hop-ceph-op/Screenshot_1703.png)

Chạy câu lệnh nội dùn trong file tải về từ công cụ tính toán của CEPH.

```
ceph osd pool create backup 64
ceph osd pool set backup size 2
while [ $(ceph -s | grep creating -c) -gt 0 ]; do echo -n .;sleep 1; done

ceph osd pool create volumes 1024
ceph osd pool set volumes size 2
while [ $(ceph -s | grep creating -c) -gt 0 ]; do echo -n .;sleep 1; done

ceph osd pool create vms 64
ceph osd pool set vms size 2
while [ $(ceph -s | grep creating -c) -gt 0 ]; do echo -n .;sleep 1; done

ceph osd pool create images 128
ceph osd pool set images size 2
while [ $(ceph -s | grep creating -c) -gt 0 ]; do echo -n .;sleep 1; done
```

Mặc định cấu hình mặc định `mon_max_pg_per_osd = 250` chỉ cho tối đa 250 pg trên một OSD nên khi tạo số pg > 250 thì CEPH sẽ báo lỗi không khởi tạo được volume.

![](../images/tich-hop-ceph-op/Screenshot_1704.png)

Mặc định CEPH không cho xóa pool để tránh rủi ra mất mát data khi xóa nhầm pool. Để có thể xóa được pool phải chỉnh sửa file ceph config trong thư mục ceph-deploy và overwrite config tới các node khác thông qua ceph-deploy.

+ Overwrite config, key: Copy đè cả key admin sang các node CEPH khác.

```
ceph-deploy admin --overwrite-config ceph01 ceph02 ceph03
```

+ Overwrite config: Sau mỗi lần chỉnh sửa config của CEPH chỉ cần đứng trong thực mục ceph-deploy copy đè config lên các node CEPH khác.

```
ceph-deploy --overwrite-conf config push ceph01 ceph02 ceph03
```

```
vi /ceph-deploy/ceph.conf

add

mon_allow pool delete = true
```

+ Giới hạn số lượng PG trên OSDs

```
vi /ceph-deploy/ceph.conf

add

mon max pg per osd  = 500

hoặc

mon_max_pg_per_osd = 500
```
Trong config của CEPH có thể có dấu `_` hoặc không có đều OK.



+ Restart service ceph MON, ceph MGR.

```
systemctl restart ceph-mon@ceph01
systemctl restart ceph-mgr@ceph01
```

Lưu ý: 

- Tạo thành công pool.

![](../images/tich-hop-ceph-op/Screenshot_1705.png)

- Khởi tạo ban đầu trước khi sử dụng pool

```
rbd pool init volumes
rbd pool init vms
rbd pool init images
rbd pool init backup
```

- Copy cấu hình qua các node CTL, COM.

```
ssh 10.10.10.158 sudo tee /etc/ceph/ceph.conf < /etc/ceph/ceph.conf
ssh 10.10.10.159 sudo tee /etc/ceph/ceph.conf < /etc/ceph/ceph.conf
ssh 10.10.10.151 sudo tee /etc/ceph/ceph.conf < /etc/ceph/ceph.conf
```

<a name="glance"></a>
### 3.2. Tích hợp CEPH làm backend cho glance - images

**Bước 1: Thực hiện trên node CEPH**

Ở trong CEPH key chính là một thành phần quan trọng dùng để xác thực, phân quyền cho một đối tượng sở hữu key đó với các service, pool trong CEPH.

- Tạo key `glance`

```
ceph auth get-or-create client.glance mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=images' 
```

Tạo key glance cho phép `read` các thông tin service `mon`, lấy đầy đủ thông tin về service `OSD`, phần quyền `read/write/excute` đối với pool có tên là `images`.

![](../images/tich-hop-ceph-op/Screenshot_1706.png)

- Chuyển key `glance` sang node glance (glance được cài đặt trên node CTL)

```
ceph auth get-or-create client.glance | ssh 10.10.10.158 sudo tee /etc/ceph/ceph.client.glance.keyring
```

![](../images/tich-hop-ceph-op/Screenshot_1707.png)

**Bước 2: Thực hiện trên node CTL**

- Set quyền cho các `key` vừa chuyển từ node CEPH sang.

```
sudo chown glance:glance /etc/ceph/ceph.client.glance.keyring
sudo chmod 0640 /etc/ceph/ceph.client.glance.keyring
```

![](../images/tich-hop-ceph-op/Screenshot_1708.png)

**Bước 3: Thêm, chỉnh sửa cấu hinh `/etc/glance/glance-api.conf` trên node CTL**

```
[DEFAULT]
show_image_direct_url = True

[glance_store]
show_image_direct_url = True
default_store = rbd
stores = file,http,rbd
rbd_store_pool = images
rbd_store_user = glance
rbd_store_ceph_conf = /etc/ceph/ceph.conf
rbd_store_chunk_size = 8
```

![](../images/tich-hop-ceph-op/Screenshot_1710.png)


![](../images/tich-hop-ceph-op/Screenshot_1709.png)

**Bước 4: Restart lại dịch vụ glance trên node CTL**

```
systemctl restart openstack-glance-*
```

**Bước 5: Upload images và kiểm tra thực hiện trên node CTL**

```
source admin-openrc
```

```
wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img
openstack image create "cirros-ceph" \
--file cirros-0.3.4-x86_64-disk.img \
--disk-format qcow2 --container-format bare \
--public
```

+ Kiểm tra trên node CEPH

```
rbd -p images ls
```

![](../images/tich-hop-ceph-op/Screenshot_1711.png)

![](../images/tich-hop-ceph-op/Screenshot_1712.png)

<a name="cinder"></a>
### 3.3. Tích hợp CEPH làm backend cho cinder-volume và cinder-backup

**Bước 1: Thực hiện trên node CEPH**

- Di chuyển vào thưc mục `ceph-deploy`

```
cd ceph-deploy
```

- Tạo key `cinder`

```
ceph auth get-or-create client.cinder mon 'allow r, allow command "osd blacklist", allow command "blacklistop"' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=images' > ceph.client.cinder.keyring
```

![](../images/tich-hop-ceph-op/Screenshot_1713.png)

- Tạo key `cinder-backup`

```
ceph auth get-or-create client.cinder-backup mon 'profile rbd' osd 'profile rbd pool=backup' > ceph.client.cinder-backup.keyring
```

![](../images/tich-hop-ceph-op/Screenshot_1714.png)

- Chuyển key `cinder` và key `cinder-backup` sang các node cài đặt Cinder (node CTL).

```
ceph auth get-or-create client.cinder | ssh 10.10.10.158 sudo tee /etc/ceph/ceph.client.cinder.keyring

ceph auth get-or-create client.cinder-backup | ssh 10.10.10.158 sudo tee /etc/ceph/ceph.client.cinder-backup.keyring
```

![](../images/tich-hop-ceph-op/Screenshot_1715.png)

- Chuyển key `cinder` sang các node COM

```
ceph auth get-or-create client.cinder | ssh 10.10.10.159 sudo tee /etc/ceph/ceph.client.cinder.keyring
ceph auth get-or-create client.cinder | ssh 10.10.10.151 sudo tee /etc/ceph/ceph.client.cinder.keyring

ceph auth get-key client.cinder | ssh 10.10.10.159 tee /root/client.cinder
ceph auth get-key client.cinder | ssh 10.10.10.151 tee /root/client.cinder
```

![](../images/tich-hop-ceph-op/Screenshot_1716.png)

**Bước 2: Thực hiện trên node CTL**

- Set quyền cho các key

```
sudo chown cinder:cinder /etc/ceph/ceph.client.cinder*
sudo chmod 0640 /etc/ceph/ceph.client.cinder*
```

![](../images/tich-hop-ceph-op/Screenshot_1717.png)

**Bước 3: Thao tác trên node COM**

- Khởi tạo 1 `uuid` mới cho cinder (UUID sử dụng chung cho các COM nên chỉ cần tạo lần đầu tiên).

```
uuidgen
```

```
067f9fdf-e3c0-4c8b-b3b2-ed80deb8d3b5
```

- Tạo file xml cho phép Ceph RBD (Rados Block Device) xác thực với libvirt (KVM) thông qua `uuid` vừa tạo.

```
cat > ceph-secret.xml <<EOF
<secret ephemeral='no' private='no'>
<uuid>067f9fdf-e3c0-4c8b-b3b2-ed80deb8d3b5</uuid>
<usage type='ceph'>
	<name>client.cinder secret</name>
</usage>
</secret>
EOF
```

```
sudo virsh secret-define --file ceph-secret.xml
```

Đoạn xml này có ý nghĩa định nghĩa khi xác thực mà gặp key có uuid là `067f9fdf-e3c0-4c8b-b3b2-ed80deb8d3b5` thì sẽ áp dụng kiểu `ceph` cho cinder.

![](../images/tich-hop-ceph-op/Screenshot_1718.png)

- Gán giá trị của `client.cinder` cho `uuid`

```
virsh secret-set-value --secret 067f9fdf-e3c0-4c8b-b3b2-ed80deb8d3b5 --base64 $(cat /root/client.cinder)
```

![](../images/tich-hop-ceph-op/Screenshot_1719.png)



**Bước 4: Chỉnh sửa, bổ sung cấu hình trên node CTL**

```
vi /etc/cinder/cinder.conf
```

```
[DEFAULT]
notification_driver = messagingv2
enabled_backends = ceph
glance_api_version = 2
backup_driver = cinder.backup.drivers.ceph
backup_ceph_conf = /etc/ceph/ceph.conf
backup_ceph_user = cinder-backup
backup_ceph_chunk_size = 134217728
backup_ceph_pool = backups
backup_ceph_stripe_unit = 0
backup_ceph_stripe_count = 0
restore_discard_excess_bytes = true
host=ceph
```

![](../images/tich-hop-ceph-op/Screenshot_1720.png)

Thêm vào cuối file 

```
[ceph]
volume_driver = cinder.volume.drivers.rbd.RBDDriver
volume_backend_name = ceph
rbd_pool = volumes
rbd_ceph_conf = /etc/ceph/ceph.conf
rbd_flatten_volume_from_snapshot = false
rbd_max_clone_depth = 5
rbd_store_chunk_size = 4
rados_connect_timeout = -1
rbd_user = cinder
rbd_secret_uuid = 067f9fdf-e3c0-4c8b-b3b2-ed80deb8d3b5
report_discard_supported = true
```

![](../images/tich-hop-ceph-op/Screenshot_1721.png)

- Enable cinder-backup và restart dịch vụ cinder

```
systemctl enable openstack-cinder-backup.service
systemctl start openstack-cinder-backup.service
systemctl restart openstack-cinder-backup.service
systemctl status openstack-cinder-backup.service
```

- Restart lại service cinder trên node CTL

```
systemctl restart openstack-cinder-api.service openstack-cinder-volume.service openstack-cinder-scheduler.service openstack-cinder-backup.service
```

![](../images/tich-hop-ceph-op/Screenshot_1722.png)

- Tạo volume type node CTL

```
cinder type-create ceph
cinder type-key ceph set volume_backend_name=ceph
```

![](../images/tich-hop-ceph-op/Screenshot_1723.png)

**Bước 5: Restart lai dich vu nova-compute trên node COM**

```
systemctl restart openstack-nova-compute
```

![](../images/tich-hop-ceph-op/Screenshot_1724.png)

**Bước 6: Tạo volume, VM và kiểm tra**

![](../images/tich-hop-ceph-op/Screenshot_1725.png)

![](../images/tich-hop-ceph-op/Screenshot_1726.png)

**Bước 7: Tích hợp các node COM còn lại**

Sau khi thao tác cấu hình tích hợp node COM1 thực hiện cấu hình tích hớp các node COM còn lại.

**Trên node CEPH**

- Cấu hình SSH key, chuyển key và nhập password cho Compute

```
ssh-copy-id root@10.10.10.151
```

![](../images/tich-hop-ceph-op/Screenshot_1727.png)

- Chuyển key `cinder` tới node `Compute`

```
ceph auth get-or-create client.cinder | ssh 10.10.10.151 sudo tee /etc/ceph/ceph.client.cinder.keyring
ceph auth get-key client.cinder | ssh 10.10.10.151 tee /root/client.cinder.key
```

![](../images/tich-hop-ceph-op/Screenshot_1728.png)

**Tên node COM**

- Tạo file secret key

```
cat > secret.xml <<EOF
<secret ephemeral='no' private='no'>
  <uuid>067f9fdf-e3c0-4c8b-b3b2-ed80deb8d3b5</uuid>
  <usage type='ceph'>
    <name>client.cinder secret</name>
  </usage>
</secret>
EOF
```

```
sudo virsh secret-define --file secret.xml
```

- Gán giá trị cho file cinder key

```
virsh secret-set-value --secret 067f9fdf-e3c0-4c8b-b3b2-ed80deb8d3b5 --base64 $(cat client.cinder.key)
```

![](../images/tich-hop-ceph-op/Screenshot_1729.png)

UUID `067f9fdf-e3c0-4c8b-b3b2-ed80deb8d3b5` phải trung với UUID được cấu hình tại option rbd_secret_uuid trên file  `/etc/cinder/cinder.conf` của node Controller. 

- Restart lại service nova

```
systemctl restart openstack-nova-compute 
```

<a name="nova"></a>
## 3.4. Tích hợp CEPH làm backend cho nova-compute

Mặc định các VM được tạo từ images sẽ lưu file disk ngay chính trên node COM, Việc tích hợp này cho phép file disk này được tạo 1 symlink lưu trữ dưới Ceph.

**Bước 1: Thực hiện trên node CEPH**

- Tạo `keyring` cho `nova`

```
ceph auth get-or-create client.nova mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=vms, allow rx pool=images' -o /etc/ceph/ceph.client.nova.keyring 
```

![](../images/tich-hop-ceph-op/Screenshot_1730.png)

- Copy key `nova` sang các node `COM`

```
ceph auth get-or-create client.nova | ssh 10.10.10.159 sudo tee /etc/ceph/ceph.client.nova.keyring
ceph auth get-or-create client.nova | ssh 10.10.10.151 sudo tee /etc/ceph/ceph.client.nova.keyring

ceph auth get-key client.nova | ssh 10.10.10.159 tee /root/client.nova
ceph auth get-key client.nova | ssh 10.10.10.151 tee /root/client.nova
```

![](../images/tich-hop-ceph-op/Screenshot_1731.png)

**Bước 2: Thao tác trên node COM**

- Set quyền key trên node COM

```
chgrp nova /etc/ceph/ceph.client.nova.keyring
chmod 0640 /etc/ceph/ceph.client.nova.keyring
```

- Tạo key UUID

```
uuidgen
```

```
256f95f0-c5e8-408a-ac44-7497dbb1f6ff
```
![](../images/tich-hop-ceph-op/Screenshot_1732.png)

UUID sử dụng chung cho các COM nên chỉ cần tạo lần đầu tiên.

- Tạo file xml cho phép Ceph RBD (Rados Block Device) xác thực với libvirt thông qua uuid vừa tạo

```
cat << EOF > nova-ceph.xml
<secret ephemeral="no" private="no">
  <uuid>256f95f0-c5e8-408a-ac44-7497dbb1f6ff</uuid>
  <usage type="ceph">
    <name>client.nova secret</name>
  </usage>
</secret>
EOF
```

```
sudo virsh secret-define --file nova-ceph.xml
```

Đoạn xml này có ý nghĩa định nghĩa khi xác thực mà gặp key có uuid là `256f95f0-c5e8-408a-ac44-7497dbb1f6ff` thì sẽ áp dụng kiểu `ceph` cho nova.

![](../images/tich-hop-ceph-op/Screenshot_1733.png)

- Gán giá trị của `client.nova` cho `uuid`

```
virsh secret-set-value --secret 256f95f0-c5e8-408a-ac44-7497dbb1f6ff --base64 $(cat /root/client.nova)
```

![](../images/tich-hop-ceph-op/Screenshot_1734.png)

**Bước 4: Chỉnh sửa, bổ sung cấu hình trên node COM**

Chỉnh sửa nova.conf trên COM `/etc/nova/nova.conf`

```
[libvirt]
images_rbd_pool=vms
images_type=rbd
rbd_secret_uuid=256f95f0-c5e8-408a-ac44-7497dbb1f6ff
rbd_user=nova
images_rbd_ceph_conf = /etc/ceph/ceph.conf
```

![](../images/tich-hop-ceph-op/Screenshot_1735.png)

- Restart service nova

```
systemctl restart openstack-nova-compute 
```

**Bước 5: Tạo VM từ images để kiểm tra**

```
rbd -p vms ls
rbd -p compute info name_disk
```

![](../images/tich-hop-ceph-op/Screenshot_1736.png)

**Bước 6: Tích hợp các node COM còn lại**

Sau khi thao tác cấu hình tích hợp node COM1 thực hiện cấu hình tích hớp các node COM còn lại.

**Trên node CEPH**

- Cấu hình SSH key, chuyển key và nhập password cho Compute

Đã thực hiện ở bước tích hợp cinder trước có thể bỏ qua `ssh-copy-id`

```
ssh-copy-id root@10.10.10.151
```

- Chuyển key `nova` tới node Compute

```
ceph auth get-or-create client.nova | ssh 10.10.10.151 sudo tee /etc/ceph/ceph.client.nova.keyring
ceph auth get-key client.nova | ssh 10.10.10.151 tee /root/client.nova.key
```

![](../images/tich-hop-ceph-op/Screenshot_1737.png)

**Trên node COM**

- Tạo file secret key

```
cat << EOF > nova-ceph.xml
<secret ephemeral="no" private="no">
  <uuid>256f95f0-c5e8-408a-ac44-7497dbb1f6ff</uuid>
  <usage type="ceph">
    <name>client.nova secret</name>
  </usage>
</secret>
EOF
```

```
sudo virsh secret-define --file nova-ceph.xml
```

- Gán giá trị của `client.nova` cho `uuid`

```
virsh secret-set-value --secret 256f95f0-c5e8-408a-ac44-7497dbb1f6ff --base64 $(cat /root/client.nova)
```

![](../images/tich-hop-ceph-op/Screenshot_1738.png)

- Chỉnh sửa nova.conf trên COM mới /etc/nova/nova.conf.

```
[libvirt]
images_rbd_pool=vms
images_type=rbd
rbd_secret_uuid=256f95f0-c5e8-408a-ac44-7497dbb1f6ff
rbd_user=nova
images_rbd_ceph_conf = /etc/ceph/ceph.conf
```

![](../images/tich-hop-ceph-op/Screenshot_1739.png)

- Restart service nova

```
systemctl restart openstack-nova-compute 
```

- Tạo VM trên node COM mới tích hợp để kiểm tra.

![](../images/tich-hop-ceph-op/Screenshot_1740.png)

![](../images/tich-hop-ceph-op/Screenshot_1741.png)

### Tham khảo

https://github.com/uncelvel/tutorial-ceph/blob/master/docs/operating/ceph-vs-openstack.md